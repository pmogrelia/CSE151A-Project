{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYIJBf74ZduK"
      },
      "source": [
        "# Milestone 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Link to our [ReadME](README.md)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxDRjzHuaCox"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ociYZrm-aIMy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv('main.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset shape: (7062606, 120)\n",
            "\n",
            "Column names:\n",
            "['MI_dir_L5_weight', 'MI_dir_L5_mean', 'MI_dir_L5_variance', 'MI_dir_L3_weight', 'MI_dir_L3_mean', 'MI_dir_L3_variance', 'MI_dir_L1_weight', 'MI_dir_L1_mean', 'MI_dir_L1_variance', 'MI_dir_L0.1_weight', 'MI_dir_L0.1_mean', 'MI_dir_L0.1_variance', 'MI_dir_L0.01_weight', 'MI_dir_L0.01_mean', 'MI_dir_L0.01_variance', 'H_L5_weight', 'H_L5_mean', 'H_L5_variance', 'H_L3_weight', 'H_L3_mean', 'H_L3_variance', 'H_L1_weight', 'H_L1_mean', 'H_L1_variance', 'H_L0.1_weight', 'H_L0.1_mean', 'H_L0.1_variance', 'H_L0.01_weight', 'H_L0.01_mean', 'H_L0.01_variance', 'HH_L5_weight', 'HH_L5_mean', 'HH_L5_std', 'HH_L5_magnitude', 'HH_L5_radius', 'HH_L5_covariance', 'HH_L5_pcc', 'HH_L3_weight', 'HH_L3_mean', 'HH_L3_std', 'HH_L3_magnitude', 'HH_L3_radius', 'HH_L3_covariance', 'HH_L3_pcc', 'HH_L1_weight', 'HH_L1_mean', 'HH_L1_std', 'HH_L1_magnitude', 'HH_L1_radius', 'HH_L1_covariance', 'HH_L1_pcc', 'HH_L0.1_weight', 'HH_L0.1_mean', 'HH_L0.1_std', 'HH_L0.1_magnitude', 'HH_L0.1_radius', 'HH_L0.1_covariance', 'HH_L0.1_pcc', 'HH_L0.01_weight', 'HH_L0.01_mean', 'HH_L0.01_std', 'HH_L0.01_magnitude', 'HH_L0.01_radius', 'HH_L0.01_covariance', 'HH_L0.01_pcc', 'HH_jit_L5_weight', 'HH_jit_L5_mean', 'HH_jit_L5_variance', 'HH_jit_L3_weight', 'HH_jit_L3_mean', 'HH_jit_L3_variance', 'HH_jit_L1_weight', 'HH_jit_L1_mean', 'HH_jit_L1_variance', 'HH_jit_L0.1_weight', 'HH_jit_L0.1_mean', 'HH_jit_L0.1_variance', 'HH_jit_L0.01_weight', 'HH_jit_L0.01_mean', 'HH_jit_L0.01_variance', 'HpHp_L5_weight', 'HpHp_L5_mean', 'HpHp_L5_std', 'HpHp_L5_magnitude', 'HpHp_L5_radius', 'HpHp_L5_covariance', 'HpHp_L5_pcc', 'HpHp_L3_weight', 'HpHp_L3_mean', 'HpHp_L3_std', 'HpHp_L3_magnitude', 'HpHp_L3_radius', 'HpHp_L3_covariance', 'HpHp_L3_pcc', 'HpHp_L1_weight', 'HpHp_L1_mean', 'HpHp_L1_std', 'HpHp_L1_magnitude', 'HpHp_L1_radius', 'HpHp_L1_covariance', 'HpHp_L1_pcc', 'HpHp_L0.1_weight', 'HpHp_L0.1_mean', 'HpHp_L0.1_std', 'HpHp_L0.1_magnitude', 'HpHp_L0.1_radius', 'HpHp_L0.1_covariance', 'HpHp_L0.1_pcc', 'HpHp_L0.01_weight', 'HpHp_L0.01_mean', 'HpHp_L0.01_std', 'HpHp_L0.01_magnitude', 'HpHp_L0.01_radius', 'HpHp_L0.01_covariance', 'HpHp_L0.01_pcc', 'Device', 'is_attack', 'Botnet_Type', 'Attack_Type', 'Label_numeric']\n",
            "\n",
            "First few rows:\n",
            "   MI_dir_L5_weight  MI_dir_L5_mean  MI_dir_L5_variance  MI_dir_L3_weight  \\\n",
            "0          1.000000            60.0                 0.0          1.000000   \n",
            "1          1.000000            60.0                 0.0          1.000000   \n",
            "2          1.000000            60.0                 0.0          1.000000   \n",
            "3          1.000000           590.0                 0.0          1.000000   \n",
            "4          1.927179           590.0                 0.0          1.955648   \n",
            "\n",
            "   MI_dir_L3_mean  MI_dir_L3_variance  MI_dir_L1_weight  MI_dir_L1_mean  \\\n",
            "0            60.0                 0.0          1.000000            60.0   \n",
            "1            60.0                 0.0          1.000000            60.0   \n",
            "2            60.0                 0.0          1.000000            60.0   \n",
            "3           590.0                 0.0          1.000000           590.0   \n",
            "4           590.0                 0.0          1.984992           590.0   \n",
            "\n",
            "   MI_dir_L1_variance  MI_dir_L0.1_weight  ...  HpHp_L0.01_std  \\\n",
            "0                 0.0            1.000000  ...    0.000000e+00   \n",
            "1                 0.0            1.000000  ...    9.540000e-07   \n",
            "2                 0.0            1.000000  ...    0.000000e+00   \n",
            "3                 0.0            1.000000  ...    9.199164e+01   \n",
            "4                 0.0            1.998489  ...    1.108120e+02   \n",
            "\n",
            "   HpHp_L0.01_magnitude  HpHp_L0.01_radius  HpHp_L0.01_covariance  \\\n",
            "0             60.000000       0.000000e+00                    0.0   \n",
            "1             60.000000       9.090000e-13                    0.0   \n",
            "2             60.000000       0.000000e+00                    0.0   \n",
            "3            388.850426       8.462461e+03                    0.0   \n",
            "4            418.293119       1.227931e+04                    0.0   \n",
            "\n",
            "   HpHp_L0.01_pcc                                    Device  is_attack  \\\n",
            "0             0.0  SimpleHome_XCS7_1003_WHT_Security_Camera          0   \n",
            "1             0.0  SimpleHome_XCS7_1003_WHT_Security_Camera          0   \n",
            "2             0.0  SimpleHome_XCS7_1003_WHT_Security_Camera          0   \n",
            "3             0.0  SimpleHome_XCS7_1003_WHT_Security_Camera          0   \n",
            "4             0.0  SimpleHome_XCS7_1003_WHT_Security_Camera          0   \n",
            "\n",
            "   Botnet_Type  Attack_Type  Label_numeric  \n",
            "0       Benign       Benign              0  \n",
            "1       Benign       Benign              0  \n",
            "2       Benign       Benign              0  \n",
            "3       Benign       Benign              0  \n",
            "4       Benign       Benign              0  \n",
            "\n",
            "[5 rows x 120 columns]\n",
            "\n",
            "Data types:\n",
            "float64    115\n",
            "object       3\n",
            "int64        2\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Target distribution:\n",
            "is_attack\n",
            "1    6506674\n",
            "0     555932\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Device distribution:\n",
            "Device\n",
            "Philips_B120N10_Baby_Monitor                1098677\n",
            "Danmini_Doorbell                            1018298\n",
            "SimpleHome_XCS7_1002_WHT_Security_Camera     863056\n",
            "SimpleHome_XCS7_1003_WHT_Security_Camera     850826\n",
            "Provision_PT_838_Security_Camera             836891\n",
            "Ecobee_Thermostat                            835876\n",
            "Provision_PT_737E_Security_Camera            828260\n",
            "Samsung_SNH_1011_N_Webcam                    375222\n",
            "Ennio_Doorbell                               355500\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Examine the dataset structure\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "print(\"\\nColumn names:\")\n",
        "print(df.columns.tolist())\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(df.head())\n",
        "print(\"\\nData types:\")\n",
        "print(df.dtypes.value_counts())\n",
        "print(\"\\nTarget distribution:\")\n",
        "print(df['is_attack'].value_counts())\n",
        "print(\"\\nDevice distribution:\")\n",
        "print(df['Device'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "Following the preprocessing steps outlined in the README."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Missing values per column:\n",
            "0\n",
            "\n",
            "Number of duplicate rows:\n",
            "157779\n",
            "\n",
            "Found 25 weight columns\n",
            "Rows with total weight < 0.001: 0\n",
            "Min total weight: 25.0\n",
            "Max total weight: 252403.3509269681\n"
          ]
        }
      ],
      "source": [
        "# Check for missing values and duplicates\n",
        "print(\"Missing values per column:\")\n",
        "print(df.isnull().sum().sum())\n",
        "print(\"\\nNumber of duplicate rows:\")\n",
        "print(df.duplicated().sum())\n",
        "\n",
        "# Check for any rows with minimal weight (close to 0)\n",
        "weight_cols = [col for col in df.columns if 'weight' in col]\n",
        "print(f\"\\nFound {len(weight_cols)} weight columns\")\n",
        "\n",
        "# Check for rows where all weights are very small\n",
        "if weight_cols:\n",
        "    min_weights = df[weight_cols].sum(axis=1)\n",
        "    print(f\"Rows with total weight < 0.001: {(min_weights < 0.001).sum()}\")\n",
        "    print(f\"Min total weight: {min_weights.min()}\")\n",
        "    print(f\"Max total weight: {min_weights.max()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original shape: (7062606, 120)\n",
            "After removing duplicates: (6904827, 120)\n",
            "Removed 157779 duplicate rows\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Remove duplicates (memory efficient)\n",
        "print(f\"Original shape: {df.shape}\")\n",
        "df_clean = df.drop_duplicates()\n",
        "print(f\"After removing duplicates: {df_clean.shape}\")\n",
        "print(f\"Removed {df.shape[0] - df_clean.shape[0]} duplicate rows\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Remove rows with minimal weight (very cheap computation)\n",
        "weight_cols = [col for col in df_clean.columns if 'weight' in col]\n",
        "if weight_cols:\n",
        "    # Calculate total weight per row (vectorized operation)\n",
        "    total_weight = df_clean[weight_cols].sum(axis=1)\n",
        "    \n",
        "    # Keep only rows with meaningful weight\n",
        "    meaningful_weight_mask = total_weight >= 0.001\n",
        "    df_clean = df_clean[meaningful_weight_mask]\n",
        "    \n",
        "    print(f\"Removed {(~meaningful_weight_mask).sum()} rows with minimal weight\")\n",
        "    print(f\"Shape after weight filtering: {df_clean.shape}\")\n",
        "else:\n",
        "    print(\"No weight columns found - skipping weight filtering\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 4: Simple data balancing (cheap sampling approach)\n",
        "print(\"\\nClass distribution before balancing:\")\n",
        "print(df_clean['is_attack'].value_counts())\n",
        "\n",
        "# Get class counts\n",
        "attack_count = (df_clean['is_attack'] == 1).sum()\n",
        "benign_count = (df_clean['is_attack'] == 0).sum()\n",
        "\n",
        "# Simple downsampling of majority class to balance\n",
        "if benign_count > attack_count * 2:  # Only balance if very imbalanced\n",
        "    # Downsample benign to 2x attack count (still manageable for computation)\n",
        "    target_benign = min(attack_count * 2, benign_count)\n",
        "    \n",
        "    benign_data = df_clean[df_clean['is_attack'] == 0].sample(n=target_benign, random_state=42)\n",
        "    attack_data = df_clean[df_clean['is_attack'] == 1]\n",
        "    \n",
        "    df_balanced = pd.concat([benign_data, attack_data], ignore_index=True)\n",
        "    \n",
        "    print(f\"\\nBalanced dataset shape: {df_balanced.shape}\")\n",
        "    print(\"Class distribution after balancing:\")\n",
        "    print(df_balanced['is_attack'].value_counts())\n",
        "else:\n",
        "    df_balanced = df_clean.copy()\n",
        "    print(\"Data is reasonably balanced - no sampling needed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 5: Simple normalization (min-max scaling)\n",
        "# Only normalize numeric feature columns, preserve categorical\n",
        "numeric_feature_cols = [col for col in df_balanced.columns \n",
        "                       if col not in ['Device', 'is_attack'] and df_balanced[col].dtype in ['float64', 'int64']]\n",
        "\n",
        "print(f\"Normalizing {len(numeric_feature_cols)} numeric columns...\")\n",
        "\n",
        "# Simple min-max scaling (vectorized operation)\n",
        "df_processed = df_balanced.copy()\n",
        "for col in numeric_feature_cols:\n",
        "    col_min = df_processed[col].min()\n",
        "    col_max = df_processed[col].max()\n",
        "    \n",
        "    # Avoid division by zero\n",
        "    if col_max != col_min:\n",
        "        df_processed[col] = (df_processed[col] - col_min) / (col_max - col_min)\n",
        "    else:\n",
        "        df_processed[col] = 0  # All values are the same\n",
        "\n",
        "print(f\"\\nFinal preprocessed dataset shape: {df_processed.shape}\")\n",
        "print(\"\\nPreprocessing complete!\")\n",
        "print(\"\\nFinal class distribution:\")\n",
        "print(df_processed['is_attack'].value_counts())\n",
        "\n",
        "# Save memory by deleting intermediate dataframes\n",
        "del df_clean, df_balanced"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
