{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYIJBf74ZduK"
      },
      "source": [
        "# Milestone 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Link to our [ReadME](README.md)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxDRjzHuaCox"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ociYZrm-aIMy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'df' not in globals():\n",
        "    import pandas as pd\n",
        "    df = pd.read_csv('main.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset shape: (7062606, 120)\n",
            "\n",
            "Column names:\n",
            "['MI_dir_L5_weight', 'MI_dir_L5_mean', 'MI_dir_L5_variance', 'MI_dir_L3_weight', 'MI_dir_L3_mean', 'MI_dir_L3_variance', 'MI_dir_L1_weight', 'MI_dir_L1_mean', 'MI_dir_L1_variance', 'MI_dir_L0.1_weight', 'MI_dir_L0.1_mean', 'MI_dir_L0.1_variance', 'MI_dir_L0.01_weight', 'MI_dir_L0.01_mean', 'MI_dir_L0.01_variance', 'H_L5_weight', 'H_L5_mean', 'H_L5_variance', 'H_L3_weight', 'H_L3_mean', 'H_L3_variance', 'H_L1_weight', 'H_L1_mean', 'H_L1_variance', 'H_L0.1_weight', 'H_L0.1_mean', 'H_L0.1_variance', 'H_L0.01_weight', 'H_L0.01_mean', 'H_L0.01_variance', 'HH_L5_weight', 'HH_L5_mean', 'HH_L5_std', 'HH_L5_magnitude', 'HH_L5_radius', 'HH_L5_covariance', 'HH_L5_pcc', 'HH_L3_weight', 'HH_L3_mean', 'HH_L3_std', 'HH_L3_magnitude', 'HH_L3_radius', 'HH_L3_covariance', 'HH_L3_pcc', 'HH_L1_weight', 'HH_L1_mean', 'HH_L1_std', 'HH_L1_magnitude', 'HH_L1_radius', 'HH_L1_covariance', 'HH_L1_pcc', 'HH_L0.1_weight', 'HH_L0.1_mean', 'HH_L0.1_std', 'HH_L0.1_magnitude', 'HH_L0.1_radius', 'HH_L0.1_covariance', 'HH_L0.1_pcc', 'HH_L0.01_weight', 'HH_L0.01_mean', 'HH_L0.01_std', 'HH_L0.01_magnitude', 'HH_L0.01_radius', 'HH_L0.01_covariance', 'HH_L0.01_pcc', 'HH_jit_L5_weight', 'HH_jit_L5_mean', 'HH_jit_L5_variance', 'HH_jit_L3_weight', 'HH_jit_L3_mean', 'HH_jit_L3_variance', 'HH_jit_L1_weight', 'HH_jit_L1_mean', 'HH_jit_L1_variance', 'HH_jit_L0.1_weight', 'HH_jit_L0.1_mean', 'HH_jit_L0.1_variance', 'HH_jit_L0.01_weight', 'HH_jit_L0.01_mean', 'HH_jit_L0.01_variance', 'HpHp_L5_weight', 'HpHp_L5_mean', 'HpHp_L5_std', 'HpHp_L5_magnitude', 'HpHp_L5_radius', 'HpHp_L5_covariance', 'HpHp_L5_pcc', 'HpHp_L3_weight', 'HpHp_L3_mean', 'HpHp_L3_std', 'HpHp_L3_magnitude', 'HpHp_L3_radius', 'HpHp_L3_covariance', 'HpHp_L3_pcc', 'HpHp_L1_weight', 'HpHp_L1_mean', 'HpHp_L1_std', 'HpHp_L1_magnitude', 'HpHp_L1_radius', 'HpHp_L1_covariance', 'HpHp_L1_pcc', 'HpHp_L0.1_weight', 'HpHp_L0.1_mean', 'HpHp_L0.1_std', 'HpHp_L0.1_magnitude', 'HpHp_L0.1_radius', 'HpHp_L0.1_covariance', 'HpHp_L0.1_pcc', 'HpHp_L0.01_weight', 'HpHp_L0.01_mean', 'HpHp_L0.01_std', 'HpHp_L0.01_magnitude', 'HpHp_L0.01_radius', 'HpHp_L0.01_covariance', 'HpHp_L0.01_pcc', 'Device', 'is_attack', 'Botnet_Type', 'Attack_Type', 'Label_numeric']\n",
            "\n",
            "First few rows:\n",
            "   MI_dir_L5_weight  MI_dir_L5_mean  MI_dir_L5_variance  MI_dir_L3_weight  \\\n",
            "0          1.000000            60.0                 0.0          1.000000   \n",
            "1          1.000000            60.0                 0.0          1.000000   \n",
            "2          1.000000            60.0                 0.0          1.000000   \n",
            "3          1.000000           590.0                 0.0          1.000000   \n",
            "4          1.927179           590.0                 0.0          1.955648   \n",
            "\n",
            "   MI_dir_L3_mean  MI_dir_L3_variance  MI_dir_L1_weight  MI_dir_L1_mean  \\\n",
            "0            60.0                 0.0          1.000000            60.0   \n",
            "1            60.0                 0.0          1.000000            60.0   \n",
            "2            60.0                 0.0          1.000000            60.0   \n",
            "3           590.0                 0.0          1.000000           590.0   \n",
            "4           590.0                 0.0          1.984992           590.0   \n",
            "\n",
            "   MI_dir_L1_variance  MI_dir_L0.1_weight  ...  HpHp_L0.01_std  \\\n",
            "0                 0.0            1.000000  ...    0.000000e+00   \n",
            "1                 0.0            1.000000  ...    9.540000e-07   \n",
            "2                 0.0            1.000000  ...    0.000000e+00   \n",
            "3                 0.0            1.000000  ...    9.199164e+01   \n",
            "4                 0.0            1.998489  ...    1.108120e+02   \n",
            "\n",
            "   HpHp_L0.01_magnitude  HpHp_L0.01_radius  HpHp_L0.01_covariance  \\\n",
            "0             60.000000       0.000000e+00                    0.0   \n",
            "1             60.000000       9.090000e-13                    0.0   \n",
            "2             60.000000       0.000000e+00                    0.0   \n",
            "3            388.850426       8.462461e+03                    0.0   \n",
            "4            418.293119       1.227931e+04                    0.0   \n",
            "\n",
            "   HpHp_L0.01_pcc                                    Device  is_attack  \\\n",
            "0             0.0  SimpleHome_XCS7_1003_WHT_Security_Camera          0   \n",
            "1             0.0  SimpleHome_XCS7_1003_WHT_Security_Camera          0   \n",
            "2             0.0  SimpleHome_XCS7_1003_WHT_Security_Camera          0   \n",
            "3             0.0  SimpleHome_XCS7_1003_WHT_Security_Camera          0   \n",
            "4             0.0  SimpleHome_XCS7_1003_WHT_Security_Camera          0   \n",
            "\n",
            "   Botnet_Type  Attack_Type  Label_numeric  \n",
            "0       Benign       Benign              0  \n",
            "1       Benign       Benign              0  \n",
            "2       Benign       Benign              0  \n",
            "3       Benign       Benign              0  \n",
            "4       Benign       Benign              0  \n",
            "\n",
            "[5 rows x 120 columns]\n",
            "\n",
            "Data types:\n",
            "float64    115\n",
            "object       3\n",
            "int64        2\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Target distribution:\n",
            "is_attack\n",
            "1    6506674\n",
            "0     555932\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Device distribution:\n",
            "Device\n",
            "Philips_B120N10_Baby_Monitor                1098677\n",
            "Danmini_Doorbell                            1018298\n",
            "SimpleHome_XCS7_1002_WHT_Security_Camera     863056\n",
            "SimpleHome_XCS7_1003_WHT_Security_Camera     850826\n",
            "Provision_PT_838_Security_Camera             836891\n",
            "Ecobee_Thermostat                            835876\n",
            "Provision_PT_737E_Security_Camera            828260\n",
            "Samsung_SNH_1011_N_Webcam                    375222\n",
            "Ennio_Doorbell                               355500\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Examine the dataset structure\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "print(\"\\nColumn names:\")\n",
        "print(df.columns.tolist())\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(df.head())\n",
        "print(\"\\nData types:\")\n",
        "print(df.dtypes.value_counts())\n",
        "print(\"\\nTarget distribution:\")\n",
        "print(df['is_attack'].value_counts())\n",
        "print(\"\\nDevice distribution:\")\n",
        "print(df['Device'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "Following the preprocessing steps outlined in the README."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Missing values per column:\n",
            "0\n",
            "\n",
            "Number of duplicate rows:\n",
            "157779\n",
            "\n",
            "Found 25 weight columns\n",
            "Rows with total weight < 0.001: 0\n",
            "Min total weight: 25.0\n",
            "Max total weight: 252403.3509269681\n"
          ]
        }
      ],
      "source": [
        "# Check for missing values and duplicates\n",
        "print(\"Missing values per column:\")\n",
        "print(df.isnull().sum().sum())\n",
        "print(\"\\nNumber of duplicate rows:\")\n",
        "print(df.duplicated().sum())\n",
        "\n",
        "# Check for any rows with minimal weight (close to 0)\n",
        "weight_cols = [col for col in df.columns if 'weight' in col]\n",
        "print(f\"\\nFound {len(weight_cols)} weight columns\")\n",
        "\n",
        "# Check for rows where all weights are very small\n",
        "if weight_cols:\n",
        "    min_weights = df[weight_cols].sum(axis=1)\n",
        "    print(f\"Rows with total weight < 0.001: {(min_weights < 0.001).sum()}\")\n",
        "    print(f\"Min total weight: {min_weights.min()}\")\n",
        "    print(f\"Max total weight: {min_weights.max()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original shape: (7062606, 120)\n",
            "After removing duplicates: (6904827, 120)\n",
            "Removed 157779 duplicate rows\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Remove duplicates\n",
        "print(f\"Original shape: {df.shape}\")\n",
        "df_clean = df.drop_duplicates()\n",
        "print(f\"After removing duplicates: {df_clean.shape}\")\n",
        "print(f\"Removed {df.shape[0] - df_clean.shape[0]} duplicate rows\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Class distribution before balancing:\n",
            "is_attack\n",
            "1    6391327\n",
            "0     513500\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Imbalance ratio: 12.45:1\n",
            "\n",
            "Balanced to 513500 samples per class\n",
            "Balanced dataset shape: (1027000, 120)\n",
            "Class distribution after balancing:\n",
            "is_attack\n",
            "0    513500\n",
            "1    513500\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Step 4: Balanced sampling approach\n",
        "print(\"\\nClass distribution before balancing:\")\n",
        "class_counts = df_clean['is_attack'].value_counts()\n",
        "print(class_counts)\n",
        "\n",
        "# Calculate class imbalance ratio\n",
        "attack_count = class_counts[1]\n",
        "benign_count = class_counts[0]\n",
        "imbalance_ratio = max(attack_count, benign_count) / min(attack_count, benign_count)\n",
        "\n",
        "print(f\"\\nImbalance ratio: {imbalance_ratio:.2f}:1\")\n",
        "\n",
        "# Only balance if significantly imbalanced (>3:1 ratio)\n",
        "if imbalance_ratio > 3.0:\n",
        "    # Use the minority class size as target for both classes\n",
        "    target_size = min(attack_count, benign_count)\n",
        "    \n",
        "    # Sample equal amounts from both classes\n",
        "    benign_data = df_clean[df_clean['is_attack'] == 0].sample(n=target_size, random_state=42)\n",
        "    attack_data = df_clean[df_clean['is_attack'] == 1].sample(n=target_size, random_state=42)\n",
        "    \n",
        "    df_balanced = pd.concat([benign_data, attack_data], ignore_index=True)\n",
        "    \n",
        "    print(f\"\\nBalanced to {target_size} samples per class\")\n",
        "    print(f\"Balanced dataset shape: {df_balanced.shape}\")\n",
        "    print(\"Class distribution after balancing:\")\n",
        "    print(df_balanced['is_attack'].value_counts())\n",
        "else:\n",
        "    df_balanced = df_clean.copy()\n",
        "    print(\"Data is reasonably balanced - no sampling needed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Normalizing 116 numeric columns...\n",
            "\n",
            "Final preprocessed dataset shape: (1027000, 120)\n",
            "\n",
            "Preprocessing complete!\n",
            "\n",
            "Final class distribution:\n",
            "is_attack\n",
            "0    513500\n",
            "1    513500\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Step 5: Simple normalization (min-max scaling)\n",
        "# Only normalize numeric feature columns, preserve categorical\n",
        "numeric_feature_cols = [col for col in df_balanced.columns \n",
        "                       if col not in ['Device', 'is_attack'] and df_balanced[col].dtype in ['float64', 'int64']]\n",
        "\n",
        "print(f\"Normalizing {len(numeric_feature_cols)} numeric columns...\")\n",
        "\n",
        "# Simple min-max scaling (vectorized operation)\n",
        "df_processed = df_balanced.copy()\n",
        "for col in numeric_feature_cols:\n",
        "    col_min = df_processed[col].min()\n",
        "    col_max = df_processed[col].max()\n",
        "    \n",
        "    # Avoid division by zero\n",
        "    if col_max != col_min:\n",
        "        df_processed[col] = (df_processed[col] - col_min) / (col_max - col_min)\n",
        "    else:\n",
        "        df_processed[col] = 0  # All values are the same\n",
        "\n",
        "print(f\"\\nFinal preprocessed dataset shape: {df_processed.shape}\")\n",
        "print(\"\\nPreprocessing complete!\")\n",
        "print(\"\\nFinal class distribution:\")\n",
        "print(df_processed['is_attack'].value_counts())\n",
        "\n",
        "# Save memory by deleting intermediate dataframes\n",
        "del df_clean, df_balanced"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Normalizing 116 numeric columns...\n",
            "\n",
            "Final preprocessed dataset shape: (7062606, 120)\n",
            "\n",
            "Preprocessing complete!\n",
            "\n",
            "Final class distribution:\n",
            "is_attack\n",
            "1    6506674\n",
            "0     555932\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Step 5: Simple normalization (min-max scaling)\n",
        "# Only normalize numeric feature columns, preserve categorical\n",
        "numeric_feature_cols = [col for col in df.columns \n",
        "                       if col not in ['Device', 'is_attack'] and df[col].dtype in ['float64', 'int64']]\n",
        "\n",
        "print(f\"Normalizing {len(numeric_feature_cols)} numeric columns...\")\n",
        "\n",
        "# Simple min-max scaling (vectorized operation)\n",
        "df_processed_full = df.copy()\n",
        "for col in numeric_feature_cols:\n",
        "    col_min = df_processed_full[col].min()\n",
        "    col_max = df_processed_full[col].max()\n",
        "    \n",
        "    # Avoid division by zero\n",
        "    if col_max != col_min:\n",
        "        df_processed_full[col] = (df_processed_full[col] - col_min) / (col_max - col_min)\n",
        "    else:\n",
        "        df_processed_full[col] = 0  # All values are the same\n",
        "\n",
        "print(f\"\\nFinal preprocessed dataset shape: {df_processed_full.shape}\")\n",
        "print(\"\\nPreprocessing complete!\")\n",
        "print(\"\\nFinal class distribution:\")\n",
        "print(df_processed_full['is_attack'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature matrix shape: (1027000, 118)\n",
            "Target vector shape: (1027000,)\n",
            "\n",
            "Train set: 821600 samples\n",
            "Test set: 205400 samples\n",
            "Train class distribution:\n",
            "is_attack\n",
            "0    410917\n",
            "1    410683\n",
            "Name: count, dtype: int64\n",
            "Test class distribution:\n",
            "is_attack\n",
            "1    102817\n",
            "0    102583\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "## Decision Tree Learning\n",
        "\n",
        "# Prepare features and target\n",
        "X = df_processed.drop(['is_attack', 'Device'], axis=1)  # Features only\n",
        "y = df_processed['is_attack']  # Target\n",
        "\n",
        "print(f\"Feature matrix shape: {X.shape}\")\n",
        "print(f\"Target vector shape: {y.shape}\")\n",
        "\n",
        "# Manual train/test split (80/20)\n",
        "np.random.seed(42)\n",
        "n_samples = len(df_processed)\n",
        "indices = np.random.permutation(n_samples)\n",
        "split_idx = int(0.8 * n_samples)\n",
        "\n",
        "train_idx = indices[:split_idx]\n",
        "test_idx = indices[split_idx:]\n",
        "\n",
        "X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
        "y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
        "\n",
        "print(f\"\\nTrain set: {X_train.shape[0]} samples\")\n",
        "print(f\"Test set: {X_test.shape[0]} samples\")\n",
        "print(f\"Train class distribution:\\n{y_train.value_counts()}\")\n",
        "print(f\"Test class distribution:\\n{y_test.value_counts()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preparing features for sklearn...\n",
            "Using 116 numeric features\n",
            "Training set shape: (821600, 116)\n",
            "Training scikit-learn Decision Tree...\n",
            "\n",
            "============================================================\n",
            "SCIKIT-LEARN DECISION TREE PERFORMANCE\n",
            "============================================================\n",
            "Accuracy:     0.9996\n",
            "Error Rate:   0.0004\n",
            "Precision:    0.9996\n",
            "Recall:       0.9996\n",
            "F1 Score:     0.9996\n",
            "\n",
            "Detailed Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Benign       1.00      1.00      1.00    102583\n",
            "      Attack       1.00      1.00      1.00    102817\n",
            "\n",
            "    accuracy                           1.00    205400\n",
            "   macro avg       1.00      1.00      1.00    205400\n",
            "weighted avg       1.00      1.00      1.00    205400\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "True Negatives:  102,537\n",
            "False Positives: 46\n",
            "False Negatives: 40\n",
            "True Positives:  102,777\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "## Scikit-Learn Decision Tree \n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Prepare features (only numeric)\n",
        "print(\"Preparing features for sklearn...\")\n",
        "numeric_cols = X_train.select_dtypes(include=[np.number]).columns\n",
        "X_train_sklearn = X_train[numeric_cols]\n",
        "X_test_sklearn = X_test[numeric_cols]\n",
        "\n",
        "print(f\"Using {len(numeric_cols)} numeric features\")\n",
        "print(f\"Training set shape: {X_train_sklearn.shape}\")\n",
        "\n",
        "# Create regularized Decision Tree to prevent overfitting\n",
        "dt_sklearn = DecisionTreeClassifier(\n",
        "    max_depth=6,                    # Limit tree depth\n",
        "    min_samples_split=1000,         # Require many samples to split\n",
        "    min_samples_leaf=500,           # Large leaf nodes\n",
        "    max_features='sqrt',            # Feature randomization\n",
        "    random_state=42,\n",
        "    class_weight='balanced'         # Handle class imbalance\n",
        ")\n",
        "\n",
        "print(\"Training scikit-learn Decision Tree...\")\n",
        "dt_sklearn.fit(X_train_sklearn, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_sklearn = dt_sklearn.predict(X_test_sklearn)\n",
        "\n",
        "# Calculate comprehensive metrics\n",
        "accuracy = accuracy_score(y_test, y_pred_sklearn)\n",
        "precision = precision_score(y_test, y_pred_sklearn)\n",
        "recall = recall_score(y_test, y_pred_sklearn)\n",
        "f1 = f1_score(y_test, y_pred_sklearn)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SCIKIT-LEARN DECISION TREE PERFORMANCE\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Accuracy:     {accuracy:.4f}\")\n",
        "print(f\"Error Rate:   {1-accuracy:.4f}\")\n",
        "print(f\"Precision:    {precision:.4f}\")\n",
        "print(f\"Recall:       {recall:.4f}\")\n",
        "print(f\"F1 Score:     {f1:.4f}\")\n",
        "\n",
        "print(\"\\nDetailed Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_sklearn, target_names=['Benign', 'Attack']))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "cm = confusion_matrix(y_test, y_pred_sklearn)\n",
        "print(f\"True Negatives:  {cm[0,0]:,}\")\n",
        "print(f\"False Positives: {cm[0,1]:,}\")\n",
        "print(f\"False Negatives: {cm[1,0]:,}\")\n",
        "print(f\"True Positives:  {cm[1,1]:,}\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preparing features for sklearn...\n",
            "Using 117 numeric features\n",
            "Training set shape: (7062606, 117)\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- is_attack\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[17], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining set shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFull_sklearn\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m y_pred_sklearn \u001b[38;5;241m=\u001b[39m \u001b[43mdt_sklearn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mFull_sklearn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Calculate comprehensive metrics\u001b[39;00m\n\u001b[1;32m     12\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(Full_sklearn[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_attack\u001b[39m\u001b[38;5;124m'\u001b[39m], y_pred_sklearn)\n",
            "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/sklearn/tree/_classes.py:530\u001b[0m, in \u001b[0;36mBaseDecisionTree.predict\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Predict class or regression value for X.\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \n\u001b[1;32m    509\u001b[0m \u001b[38;5;124;03mFor a classification model, the predicted class for each sample in X is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;124;03m    The predicted classes, or the predict values.\u001b[39;00m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    529\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 530\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_X_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m proba \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_\u001b[38;5;241m.\u001b[39mpredict(X)\n\u001b[1;32m    532\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
            "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/sklearn/tree/_classes.py:489\u001b[0m, in \u001b[0;36mBaseDecisionTree._validate_X_predict\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    488\u001b[0m     ensure_all_finite \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 489\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(X) \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m    498\u001b[0m     X\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc \u001b[38;5;129;01mor\u001b[39;00m X\u001b[38;5;241m.\u001b[39mindptr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc\n\u001b[1;32m    499\u001b[0m ):\n\u001b[1;32m    500\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/sklearn/utils/validation.py:2929\u001b[0m, in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2845\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate_data\u001b[39m(\n\u001b[1;32m   2846\u001b[0m     _estimator,\n\u001b[1;32m   2847\u001b[0m     \u001b[38;5;241m/\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2853\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params,\n\u001b[1;32m   2854\u001b[0m ):\n\u001b[1;32m   2855\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate input data and set or check feature names and counts of the input.\u001b[39;00m\n\u001b[1;32m   2856\u001b[0m \n\u001b[1;32m   2857\u001b[0m \u001b[38;5;124;03m    This helper function should be used in an estimator that requires input\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2927\u001b[0m \u001b[38;5;124;03m        validated.\u001b[39;00m\n\u001b[1;32m   2928\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2929\u001b[0m     \u001b[43m_check_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_estimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2930\u001b[0m     tags \u001b[38;5;241m=\u001b[39m get_tags(_estimator)\n\u001b[1;32m   2931\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tags\u001b[38;5;241m.\u001b[39mtarget_tags\u001b[38;5;241m.\u001b[39mrequired:\n",
            "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/sklearn/utils/validation.py:2787\u001b[0m, in \u001b[0;36m_check_feature_names\u001b[0;34m(estimator, X, reset)\u001b[0m\n\u001b[1;32m   2784\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[1;32m   2785\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 2787\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
            "\u001b[0;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- is_attack\n"
          ]
        }
      ],
      "source": [
        "print(\"Preparing features for sklearn...\")\n",
        "numeric_cols = df_processed_full.select_dtypes(include=[np.number]).columns\n",
        "Full_sklearn = df_processed_full[numeric_cols]\n",
        "\n",
        "print(f\"Using {len(numeric_cols)} numeric features\")\n",
        "print(f\"Training set shape: {Full_sklearn.shape}\")\n",
        "\n",
        "# Make predictions\n",
        "y_pred_sklearn = dt_sklearn.predict(Full_sklearn)\n",
        "\n",
        "# Calculate comprehensive metrics\n",
        "accuracy = accuracy_score(Full_sklearn['is_attack'], y_pred_sklearn)\n",
        "precision = precision_score(Full_sklearn['is_attack'], y_pred_sklearn)\n",
        "recall = recall_score(Full_sklearn['is_attack'], y_pred_sklearn)\n",
        "f1 = f1_score(Full_sklearn['is_attack'], y_pred_sklearn)\n",
        "\n",
        "print(\"SCIKIT-LEARN DECISION TREE PERFORMANCE\")\n",
        "print(f\"Accuracy:     {accuracy:.4f}\")\n",
        "print(f\"Error Rate:   {1-accuracy:.4f}\")\n",
        "print(f\"Precision:    {precision:.4f}\")\n",
        "print(f\"Recall:       {recall:.4f}\")\n",
        "print(f\"F1 Score:     {f1:.4f}\")\n",
        "\n",
        "print(\"\\nDetailed Classification Report:\")\n",
        "print(classification_report(Full_sklearn['is_attack'], y_pred_sklearn, target_names=['Benign', 'Attack']))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "cm = confusion_matrix(Full_sklearn['is_attack'], y_pred_sklearn)\n",
        "print(f\"True Negatives:  {cm[0,0]:,}\")\n",
        "print(f\"False Positives: {cm[0,1]:,}\")\n",
        "print(f\"False Negatives: {cm[1,0]:,}\")\n",
        "print(f\"True Positives:  {cm[1,1]:,}\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Performing 5-Fold Cross-Validation...\n",
            "Cross-Validation F1 Scores: ['0.9997', '0.9997', '0.9997', '0.9997', '0.9997']\n",
            "Mean CV F1 Score: 0.9997 ± 0.0000\n",
            "\n",
            "Overfitting Analysis:\n",
            "Training Accuracy: 0.9996\n",
            "Test Accuracy:     0.9996\n",
            "Gap:               -0.0000\n",
            "✅ Model shows good generalization\n",
            "\n",
            "Top 10 Most Important Features:\n",
            "           feature  importance\n",
            "    HpHp_L1_radius    0.564556\n",
            " HpHp_L0.01_weight    0.144786\n",
            "    H_L0.01_weight    0.118939\n",
            "    HH_jit_L3_mean    0.073166\n",
            "       HH_L0.1_pcc    0.049786\n",
            "     H_L0.1_weight    0.040599\n",
            "  MI_dir_L3_weight    0.007969\n",
            "MI_dir_L0.1_weight    0.000131\n",
            "    HpHp_L3_radius    0.000024\n",
            " HpHp_L3_magnitude    0.000014\n"
          ]
        }
      ],
      "source": [
        "# Cross-Validation with scikit-learn\n",
        "print(\"\\nPerforming 5-Fold Cross-Validation...\")\n",
        "cv_scores = cross_val_score(dt_sklearn, X_train_sklearn, y_train, cv=5, scoring='f1')\n",
        "\n",
        "print(f\"Cross-Validation F1 Scores: {[f'{score:.4f}' for score in cv_scores]}\")\n",
        "print(f\"Mean CV F1 Score: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
        "\n",
        "# Check for overfitting\n",
        "train_accuracy = dt_sklearn.score(X_train_sklearn, y_train)\n",
        "test_accuracy = accuracy\n",
        "\n",
        "print(f\"\\nOverfitting Analysis:\")\n",
        "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
        "print(f\"Test Accuracy:     {test_accuracy:.4f}\")\n",
        "print(f\"Gap:               {train_accuracy - test_accuracy:.4f}\")\n",
        "\n",
        "if train_accuracy - test_accuracy > 0.05:\n",
        "    print(\"⚠️  Model shows signs of overfitting\")\n",
        "else:\n",
        "    print(\"✅ Model shows good generalization\")\n",
        "\n",
        "# Feature importance (top 10)\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': X_train_sklearn.columns,\n",
        "    'importance': dt_sklearn.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(f\"\\nTop 10 Most Important Features:\")\n",
        "print(feature_importance.head(10).to_string(index=False))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
