{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYIJBf74ZduK"
      },
      "source": [
        "# Milestone 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Link to our [ReadME](README.md)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxDRjzHuaCox"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ociYZrm-aIMy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv('main.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset shape: (7062606, 120)\n",
            "\n",
            "Column names:\n",
            "['MI_dir_L5_weight', 'MI_dir_L5_mean', 'MI_dir_L5_variance', 'MI_dir_L3_weight', 'MI_dir_L3_mean', 'MI_dir_L3_variance', 'MI_dir_L1_weight', 'MI_dir_L1_mean', 'MI_dir_L1_variance', 'MI_dir_L0.1_weight', 'MI_dir_L0.1_mean', 'MI_dir_L0.1_variance', 'MI_dir_L0.01_weight', 'MI_dir_L0.01_mean', 'MI_dir_L0.01_variance', 'H_L5_weight', 'H_L5_mean', 'H_L5_variance', 'H_L3_weight', 'H_L3_mean', 'H_L3_variance', 'H_L1_weight', 'H_L1_mean', 'H_L1_variance', 'H_L0.1_weight', 'H_L0.1_mean', 'H_L0.1_variance', 'H_L0.01_weight', 'H_L0.01_mean', 'H_L0.01_variance', 'HH_L5_weight', 'HH_L5_mean', 'HH_L5_std', 'HH_L5_magnitude', 'HH_L5_radius', 'HH_L5_covariance', 'HH_L5_pcc', 'HH_L3_weight', 'HH_L3_mean', 'HH_L3_std', 'HH_L3_magnitude', 'HH_L3_radius', 'HH_L3_covariance', 'HH_L3_pcc', 'HH_L1_weight', 'HH_L1_mean', 'HH_L1_std', 'HH_L1_magnitude', 'HH_L1_radius', 'HH_L1_covariance', 'HH_L1_pcc', 'HH_L0.1_weight', 'HH_L0.1_mean', 'HH_L0.1_std', 'HH_L0.1_magnitude', 'HH_L0.1_radius', 'HH_L0.1_covariance', 'HH_L0.1_pcc', 'HH_L0.01_weight', 'HH_L0.01_mean', 'HH_L0.01_std', 'HH_L0.01_magnitude', 'HH_L0.01_radius', 'HH_L0.01_covariance', 'HH_L0.01_pcc', 'HH_jit_L5_weight', 'HH_jit_L5_mean', 'HH_jit_L5_variance', 'HH_jit_L3_weight', 'HH_jit_L3_mean', 'HH_jit_L3_variance', 'HH_jit_L1_weight', 'HH_jit_L1_mean', 'HH_jit_L1_variance', 'HH_jit_L0.1_weight', 'HH_jit_L0.1_mean', 'HH_jit_L0.1_variance', 'HH_jit_L0.01_weight', 'HH_jit_L0.01_mean', 'HH_jit_L0.01_variance', 'HpHp_L5_weight', 'HpHp_L5_mean', 'HpHp_L5_std', 'HpHp_L5_magnitude', 'HpHp_L5_radius', 'HpHp_L5_covariance', 'HpHp_L5_pcc', 'HpHp_L3_weight', 'HpHp_L3_mean', 'HpHp_L3_std', 'HpHp_L3_magnitude', 'HpHp_L3_radius', 'HpHp_L3_covariance', 'HpHp_L3_pcc', 'HpHp_L1_weight', 'HpHp_L1_mean', 'HpHp_L1_std', 'HpHp_L1_magnitude', 'HpHp_L1_radius', 'HpHp_L1_covariance', 'HpHp_L1_pcc', 'HpHp_L0.1_weight', 'HpHp_L0.1_mean', 'HpHp_L0.1_std', 'HpHp_L0.1_magnitude', 'HpHp_L0.1_radius', 'HpHp_L0.1_covariance', 'HpHp_L0.1_pcc', 'HpHp_L0.01_weight', 'HpHp_L0.01_mean', 'HpHp_L0.01_std', 'HpHp_L0.01_magnitude', 'HpHp_L0.01_radius', 'HpHp_L0.01_covariance', 'HpHp_L0.01_pcc', 'Device', 'is_attack', 'Botnet_Type', 'Attack_Type', 'Label_numeric']\n",
            "\n",
            "First few rows:\n",
            "   MI_dir_L5_weight  MI_dir_L5_mean  MI_dir_L5_variance  MI_dir_L3_weight  \\\n",
            "0          1.000000            60.0                 0.0          1.000000   \n",
            "1          1.000000            60.0                 0.0          1.000000   \n",
            "2          1.000000            60.0                 0.0          1.000000   \n",
            "3          1.000000           590.0                 0.0          1.000000   \n",
            "4          1.927179           590.0                 0.0          1.955648   \n",
            "\n",
            "   MI_dir_L3_mean  MI_dir_L3_variance  MI_dir_L1_weight  MI_dir_L1_mean  \\\n",
            "0            60.0                 0.0          1.000000            60.0   \n",
            "1            60.0                 0.0          1.000000            60.0   \n",
            "2            60.0                 0.0          1.000000            60.0   \n",
            "3           590.0                 0.0          1.000000           590.0   \n",
            "4           590.0                 0.0          1.984992           590.0   \n",
            "\n",
            "   MI_dir_L1_variance  MI_dir_L0.1_weight  ...  HpHp_L0.01_std  \\\n",
            "0                 0.0            1.000000  ...    0.000000e+00   \n",
            "1                 0.0            1.000000  ...    9.540000e-07   \n",
            "2                 0.0            1.000000  ...    0.000000e+00   \n",
            "3                 0.0            1.000000  ...    9.199164e+01   \n",
            "4                 0.0            1.998489  ...    1.108120e+02   \n",
            "\n",
            "   HpHp_L0.01_magnitude  HpHp_L0.01_radius  HpHp_L0.01_covariance  \\\n",
            "0             60.000000       0.000000e+00                    0.0   \n",
            "1             60.000000       9.090000e-13                    0.0   \n",
            "2             60.000000       0.000000e+00                    0.0   \n",
            "3            388.850426       8.462461e+03                    0.0   \n",
            "4            418.293119       1.227931e+04                    0.0   \n",
            "\n",
            "   HpHp_L0.01_pcc                                    Device  is_attack  \\\n",
            "0             0.0  SimpleHome_XCS7_1003_WHT_Security_Camera          0   \n",
            "1             0.0  SimpleHome_XCS7_1003_WHT_Security_Camera          0   \n",
            "2             0.0  SimpleHome_XCS7_1003_WHT_Security_Camera          0   \n",
            "3             0.0  SimpleHome_XCS7_1003_WHT_Security_Camera          0   \n",
            "4             0.0  SimpleHome_XCS7_1003_WHT_Security_Camera          0   \n",
            "\n",
            "   Botnet_Type  Attack_Type  Label_numeric  \n",
            "0       Benign       Benign              0  \n",
            "1       Benign       Benign              0  \n",
            "2       Benign       Benign              0  \n",
            "3       Benign       Benign              0  \n",
            "4       Benign       Benign              0  \n",
            "\n",
            "[5 rows x 120 columns]\n",
            "\n",
            "Data types:\n",
            "float64    115\n",
            "object       3\n",
            "int64        2\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Target distribution:\n",
            "is_attack\n",
            "1    6506674\n",
            "0     555932\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Device distribution:\n",
            "Device\n",
            "Philips_B120N10_Baby_Monitor                1098677\n",
            "Danmini_Doorbell                            1018298\n",
            "SimpleHome_XCS7_1002_WHT_Security_Camera     863056\n",
            "SimpleHome_XCS7_1003_WHT_Security_Camera     850826\n",
            "Provision_PT_838_Security_Camera             836891\n",
            "Ecobee_Thermostat                            835876\n",
            "Provision_PT_737E_Security_Camera            828260\n",
            "Samsung_SNH_1011_N_Webcam                    375222\n",
            "Ennio_Doorbell                               355500\n",
            "Name: count, dtype: int64\n",
            "is_attack\n",
            "1    6506674\n",
            "0     555932\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Device distribution:\n",
            "Device\n",
            "Philips_B120N10_Baby_Monitor                1098677\n",
            "Danmini_Doorbell                            1018298\n",
            "SimpleHome_XCS7_1002_WHT_Security_Camera     863056\n",
            "SimpleHome_XCS7_1003_WHT_Security_Camera     850826\n",
            "Provision_PT_838_Security_Camera             836891\n",
            "Ecobee_Thermostat                            835876\n",
            "Provision_PT_737E_Security_Camera            828260\n",
            "Samsung_SNH_1011_N_Webcam                    375222\n",
            "Ennio_Doorbell                               355500\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Examine the dataset structure\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "print(\"\\nColumn names:\")\n",
        "print(df.columns.tolist())\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(df.head())\n",
        "print(\"\\nData types:\")\n",
        "print(df.dtypes.value_counts())\n",
        "print(\"\\nTarget distribution:\")\n",
        "print(df['is_attack'].value_counts())\n",
        "print(\"\\nDevice distribution:\")\n",
        "print(df['Device'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "Following the preprocessing steps outlined in the README."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Missing values per column:\n",
            "0\n",
            "\n",
            "Number of duplicate rows:\n",
            "0\n",
            "\n",
            "Number of duplicate rows:\n",
            "157779\n",
            "\n",
            "Found 25 weight columns\n",
            "157779\n",
            "\n",
            "Found 25 weight columns\n",
            "Rows with total weight < 0.001: 0\n",
            "Min total weight: 25.0\n",
            "Max total weight: 252403.3509269681\n",
            "Rows with total weight < 0.001: 0\n",
            "Min total weight: 25.0\n",
            "Max total weight: 252403.3509269681\n"
          ]
        }
      ],
      "source": [
        "# Check for missing values and duplicates\n",
        "print(\"Missing values per column:\")\n",
        "print(df.isnull().sum().sum())\n",
        "print(\"\\nNumber of duplicate rows:\")\n",
        "print(df.duplicated().sum())\n",
        "\n",
        "# Check for any rows with minimal weight (close to 0)\n",
        "weight_cols = [col for col in df.columns if 'weight' in col]\n",
        "print(f\"\\nFound {len(weight_cols)} weight columns\")\n",
        "\n",
        "# Check for rows where all weights are very small\n",
        "if weight_cols:\n",
        "    min_weights = df[weight_cols].sum(axis=1)\n",
        "    print(f\"Rows with total weight < 0.001: {(min_weights < 0.001).sum()}\")\n",
        "    print(f\"Min total weight: {min_weights.min()}\")\n",
        "    print(f\"Max total weight: {min_weights.max()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original shape: (7062606, 120)\n",
            "After removing duplicates: (6904827, 120)\n",
            "Removed 157779 duplicate rows\n",
            "After removing duplicates: (6904827, 120)\n",
            "Removed 157779 duplicate rows\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Remove duplicates\n",
        "print(f\"Original shape: {df.shape}\")\n",
        "df_clean = df.drop_duplicates()\n",
        "print(f\"After removing duplicates: {df_clean.shape}\")\n",
        "print(f\"Removed {df.shape[0] - df_clean.shape[0]} duplicate rows\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Class distribution before balancing:\n",
            "is_attack\n",
            "1    6391327\n",
            "0     513500\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Imbalance ratio: 12.45:1\n",
            "is_attack\n",
            "1    6391327\n",
            "0     513500\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Imbalance ratio: 12.45:1\n",
            "\n",
            "Balanced to 513500 samples per class\n",
            "Balanced dataset shape: (1027000, 120)\n",
            "Class distribution after balancing:\n",
            "is_attack\n",
            "0    513500\n",
            "1    513500\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Balanced to 513500 samples per class\n",
            "Balanced dataset shape: (1027000, 120)\n",
            "Class distribution after balancing:\n",
            "is_attack\n",
            "0    513500\n",
            "1    513500\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Step 4: Balanced sampling approach\n",
        "print(\"\\nClass distribution before balancing:\")\n",
        "class_counts = df_clean['is_attack'].value_counts()\n",
        "print(class_counts)\n",
        "\n",
        "# Calculate class imbalance ratio\n",
        "attack_count = class_counts[1]\n",
        "benign_count = class_counts[0]\n",
        "imbalance_ratio = max(attack_count, benign_count) / min(attack_count, benign_count)\n",
        "\n",
        "print(f\"\\nImbalance ratio: {imbalance_ratio:.2f}:1\")\n",
        "\n",
        "# Only balance if significantly imbalanced (>3:1 ratio)\n",
        "if imbalance_ratio > 3.0:\n",
        "    # Use the minority class size as target for both classes\n",
        "    target_size = min(attack_count, benign_count)\n",
        "    \n",
        "    # Sample equal amounts from both classes\n",
        "    benign_data = df_clean[df_clean['is_attack'] == 0].sample(n=target_size, random_state=42)\n",
        "    attack_data = df_clean[df_clean['is_attack'] == 1].sample(n=target_size, random_state=42)\n",
        "    \n",
        "    df_balanced = pd.concat([benign_data, attack_data], ignore_index=True)\n",
        "    \n",
        "    print(f\"\\nBalanced to {target_size} samples per class\")\n",
        "    print(f\"Balanced dataset shape: {df_balanced.shape}\")\n",
        "    print(\"Class distribution after balancing:\")\n",
        "    print(df_balanced['is_attack'].value_counts())\n",
        "else:\n",
        "    df_balanced = df_clean.copy()\n",
        "    print(\"Data is reasonably balanced - no sampling needed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Normalizing 116 numeric columns...\n",
            "\n",
            "Final preprocessed dataset shape: (1027000, 120)\n",
            "\n",
            "Preprocessing complete!\n",
            "\n",
            "Final class distribution:\n",
            "is_attack\n",
            "0    513500\n",
            "1    513500\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Final preprocessed dataset shape: (1027000, 120)\n",
            "\n",
            "Preprocessing complete!\n",
            "\n",
            "Final class distribution:\n",
            "is_attack\n",
            "0    513500\n",
            "1    513500\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Step 5: Simple normalization (min-max scaling)\n",
        "# Only normalize numeric feature columns, preserve categorical\n",
        "numeric_feature_cols = [col for col in df_balanced.columns \n",
        "                       if col not in ['Device', 'is_attack'] and df_balanced[col].dtype in ['float64', 'int64']]\n",
        "\n",
        "print(f\"Normalizing {len(numeric_feature_cols)} numeric columns...\")\n",
        "\n",
        "# Simple min-max scaling (vectorized operation)\n",
        "df_processed = df_balanced.copy()\n",
        "for col in numeric_feature_cols:\n",
        "    col_min = df_processed[col].min()\n",
        "    col_max = df_processed[col].max()\n",
        "    \n",
        "    # Avoid division by zero\n",
        "    if col_max != col_min:\n",
        "        df_processed[col] = (df_processed[col] - col_min) / (col_max - col_min)\n",
        "    else:\n",
        "        df_processed[col] = 0  # All values are the same\n",
        "\n",
        "print(f\"\\nFinal preprocessed dataset shape: {df_processed.shape}\")\n",
        "print(\"\\nPreprocessing complete!\")\n",
        "print(\"\\nFinal class distribution:\")\n",
        "print(df_processed['is_attack'].value_counts())\n",
        "\n",
        "# Save memory by deleting intermediate dataframes\n",
        "del df_clean, df_balanced"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature matrix shape: (1027000, 118)\n",
            "Target vector shape: (1027000,)\n",
            "\n",
            "Train set: 821600 samples\n",
            "Test set: 205400 samples\n",
            "Train class distribution:\n",
            "is_attack\n",
            "0    410917\n",
            "1    410683\n",
            "Name: count, dtype: int64\n",
            "Test class distribution:\n",
            "is_attack\n",
            "1    102817\n",
            "0    102583\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Train set: 821600 samples\n",
            "Test set: 205400 samples\n",
            "Train class distribution:\n",
            "is_attack\n",
            "0    410917\n",
            "1    410683\n",
            "Name: count, dtype: int64\n",
            "Test class distribution:\n",
            "is_attack\n",
            "1    102817\n",
            "0    102583\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "## Decision Tree Learning\n",
        "\n",
        "# Prepare features and target\n",
        "X = df_processed.drop(['is_attack', 'Device'], axis=1)  # Features only\n",
        "y = df_processed['is_attack']  # Target\n",
        "\n",
        "print(f\"Feature matrix shape: {X.shape}\")\n",
        "print(f\"Target vector shape: {y.shape}\")\n",
        "\n",
        "# Manual train/test split (80/20)\n",
        "np.random.seed(42)\n",
        "n_samples = len(df_processed)\n",
        "indices = np.random.permutation(n_samples)\n",
        "split_idx = int(0.8 * n_samples)\n",
        "\n",
        "train_idx = indices[:split_idx]\n",
        "test_idx = indices[split_idx:]\n",
        "\n",
        "X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
        "y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
        "\n",
        "print(f\"\\nTrain set: {X_train.shape[0]} samples\")\n",
        "print(f\"Test set: {X_test.shape[0]} samples\")\n",
        "print(f\"Train class distribution:\\n{y_train.value_counts()}\")\n",
        "print(f\"Test class distribution:\\n{y_test.value_counts()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preparing features for sklearn...\n",
            "Using 116 numeric features\n",
            "Training set shape: (821600, 116)\n",
            "Training scikit-learn Decision Tree...\n",
            "Using 116 numeric features\n",
            "Training set shape: (821600, 116)\n",
            "Training scikit-learn Decision Tree...\n",
            "\n",
            "============================================================\n",
            "SCIKIT-LEARN DECISION TREE PERFORMANCE\n",
            "============================================================\n",
            "Accuracy:     0.9996\n",
            "Error Rate:   0.0004\n",
            "Precision:    0.9996\n",
            "Recall:       0.9996\n",
            "F1 Score:     0.9996\n",
            "\n",
            "Detailed Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Benign       1.00      1.00      1.00    102583\n",
            "      Attack       1.00      1.00      1.00    102817\n",
            "\n",
            "    accuracy                           1.00    205400\n",
            "   macro avg       1.00      1.00      1.00    205400\n",
            "weighted avg       1.00      1.00      1.00    205400\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "True Negatives:  102,537\n",
            "False Positives: 46\n",
            "False Negatives: 40\n",
            "True Positives:  102,777\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "SCIKIT-LEARN DECISION TREE PERFORMANCE\n",
            "============================================================\n",
            "Accuracy:     0.9996\n",
            "Error Rate:   0.0004\n",
            "Precision:    0.9996\n",
            "Recall:       0.9996\n",
            "F1 Score:     0.9996\n",
            "\n",
            "Detailed Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Benign       1.00      1.00      1.00    102583\n",
            "      Attack       1.00      1.00      1.00    102817\n",
            "\n",
            "    accuracy                           1.00    205400\n",
            "   macro avg       1.00      1.00      1.00    205400\n",
            "weighted avg       1.00      1.00      1.00    205400\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "True Negatives:  102,537\n",
            "False Positives: 46\n",
            "False Negatives: 40\n",
            "True Positives:  102,777\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "## Scikit-Learn Decision Tree \n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Prepare features (only numeric)\n",
        "print(\"Preparing features for sklearn...\")\n",
        "numeric_cols = X_train.select_dtypes(include=[np.number]).columns\n",
        "X_train_sklearn = X_train[numeric_cols]\n",
        "X_test_sklearn = X_test[numeric_cols]\n",
        "\n",
        "print(f\"Using {len(numeric_cols)} numeric features\")\n",
        "print(f\"Training set shape: {X_train_sklearn.shape}\")\n",
        "\n",
        "# Create regularized Decision Tree to prevent overfitting\n",
        "dt_sklearn = DecisionTreeClassifier(\n",
        "    max_depth=6,                    # Limit tree depth\n",
        "    min_samples_split=1000,         # Require many samples to split\n",
        "    min_samples_leaf=500,           # Large leaf nodes\n",
        "    max_features='sqrt',            # Feature randomization\n",
        "    random_state=42,\n",
        "    class_weight='balanced'         # Handle class imbalance\n",
        ")\n",
        "\n",
        "print(\"Training scikit-learn Decision Tree...\")\n",
        "dt_sklearn.fit(X_train_sklearn, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_sklearn = dt_sklearn.predict(X_test_sklearn)\n",
        "\n",
        "# Calculate comprehensive metrics\n",
        "accuracy = accuracy_score(y_test, y_pred_sklearn)\n",
        "precision = precision_score(y_test, y_pred_sklearn)\n",
        "recall = recall_score(y_test, y_pred_sklearn)\n",
        "f1 = f1_score(y_test, y_pred_sklearn)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SCIKIT-LEARN DECISION TREE PERFORMANCE\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Accuracy:     {accuracy:.4f}\")\n",
        "print(f\"Error Rate:   {1-accuracy:.4f}\")\n",
        "print(f\"Precision:    {precision:.4f}\")\n",
        "print(f\"Recall:       {recall:.4f}\")\n",
        "print(f\"F1 Score:     {f1:.4f}\")\n",
        "\n",
        "print(\"\\nDetailed Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_sklearn, target_names=['Benign', 'Attack']))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "cm = confusion_matrix(y_test, y_pred_sklearn)\n",
        "print(f\"True Negatives:  {cm[0,0]:,}\")\n",
        "print(f\"False Positives: {cm[0,1]:,}\")\n",
        "print(f\"False Negatives: {cm[1,0]:,}\")\n",
        "print(f\"True Positives:  {cm[1,1]:,}\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Performing 5-Fold Cross-Validation...\n",
            "Cross-Validation F1 Scores: ['0.9997', '0.9997', '0.9997', '0.9997', '0.9997']\n",
            "Mean CV F1 Score: 0.9997 ± 0.0000\n",
            "Cross-Validation F1 Scores: ['0.9997', '0.9997', '0.9997', '0.9997', '0.9997']\n",
            "Mean CV F1 Score: 0.9997 ± 0.0000\n",
            "\n",
            "Overfitting Analysis:\n",
            "Training Accuracy: 0.9996\n",
            "Test Accuracy:     0.9996\n",
            "Gap:               -0.0000\n",
            "✅ Model shows good generalization\n",
            "\n",
            "Top 10 Most Important Features:\n",
            "           feature  importance\n",
            "    HpHp_L1_radius    0.564556\n",
            " HpHp_L0.01_weight    0.144786\n",
            "    H_L0.01_weight    0.118939\n",
            "    HH_jit_L3_mean    0.073166\n",
            "       HH_L0.1_pcc    0.049786\n",
            "     H_L0.1_weight    0.040599\n",
            "  MI_dir_L3_weight    0.007969\n",
            "MI_dir_L0.1_weight    0.000131\n",
            "    HpHp_L3_radius    0.000024\n",
            " HpHp_L3_magnitude    0.000014\n",
            "\n",
            "Overfitting Analysis:\n",
            "Training Accuracy: 0.9996\n",
            "Test Accuracy:     0.9996\n",
            "Gap:               -0.0000\n",
            "✅ Model shows good generalization\n",
            "\n",
            "Top 10 Most Important Features:\n",
            "           feature  importance\n",
            "    HpHp_L1_radius    0.564556\n",
            " HpHp_L0.01_weight    0.144786\n",
            "    H_L0.01_weight    0.118939\n",
            "    HH_jit_L3_mean    0.073166\n",
            "       HH_L0.1_pcc    0.049786\n",
            "     H_L0.1_weight    0.040599\n",
            "  MI_dir_L3_weight    0.007969\n",
            "MI_dir_L0.1_weight    0.000131\n",
            "    HpHp_L3_radius    0.000024\n",
            " HpHp_L3_magnitude    0.000014\n"
          ]
        }
      ],
      "source": [
        "# Cross-Validation with scikit-learn\n",
        "print(\"\\nPerforming 5-Fold Cross-Validation...\")\n",
        "cv_scores = cross_val_score(dt_sklearn, X_train_sklearn, y_train, cv=5, scoring='f1')\n",
        "\n",
        "print(f\"Cross-Validation F1 Scores: {[f'{score:.4f}' for score in cv_scores]}\")\n",
        "print(f\"Mean CV F1 Score: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
        "\n",
        "# Check for overfitting\n",
        "train_accuracy = dt_sklearn.score(X_train_sklearn, y_train)\n",
        "test_accuracy = accuracy\n",
        "\n",
        "print(f\"\\nOverfitting Analysis:\")\n",
        "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
        "print(f\"Test Accuracy:     {test_accuracy:.4f}\")\n",
        "print(f\"Gap:               {train_accuracy - test_accuracy:.4f}\")\n",
        "\n",
        "if train_accuracy - test_accuracy > 0.05:\n",
        "    print(\"⚠️  Model shows signs of overfitting\")\n",
        "else:\n",
        "    print(\"✅ Model shows good generalization\")\n",
        "\n",
        "# Feature importance (top 10)\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': X_train_sklearn.columns,\n",
        "    'importance': dt_sklearn.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(f\"\\nTop 10 Most Important Features:\")\n",
        "print(feature_importance.head(10).to_string(index=False))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
